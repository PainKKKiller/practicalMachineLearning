---
title: "Practical Machine Learning Project"
author: "Dmitry Malugin [GitHub](https://github.com/PainKKKiller/practicalMachineLearning.git)"
output: html_document
---
## Introduction of the Data Set
This document describe the analysis done for the prediction assignment of the practical machine learning course.
We are required to assess how well barbell are performed by subjects equiped with accelerometers on the belt, arm, forearm, and dumbells.


## Download and prepare data
Load pakages. For our machine learning we'll be use randomForest, it has shown best results in my attempts


```{r warning=FALSE, results='hide'}
library(caret)
library(randomForest)

```

The first step is to load the csv file data to dataframe and analyze the type & the completion rate of the data (commands are commented to limit the output size. You can run it deleting the "#" ) :

```{r warning=FALSE}
data <- read.csv("data/pml-training.csv")
#summary(data)
#describe(data)
#sapply(data, class)
#str(data)
```

Then, data set has been split into 2 parts:

- training (70%)
- testing (30%)

```{r warning=FALSE}
detail <- data[data$new_window == "no",]
set.seed(1981)
inTrain<- createDataPartition(y=detail$classe, p=.7,list=F)
Training <-detail[inTrain,]
Testing<-detail[-inTrain,]
```

And has been done some preprocessing.

```{r warning=FALSE}
# eliminate the features that are completed NA
predictors <- sapply(1:160, function(x) all(!is.na(Training[,x]) & is.numeric(Training[,x])))
# exclude the first columns not suited as predictors
predictors[1:7] <- FALSE
num.predictors <- sum(predictors)
TrainingNoNA <- Training[ ,predictors]
# center and scale features
prepro <- preProcess(TrainingNoNA, method=c("center", "scale"))
TrainingCenSca <- predict(prepro, TrainingNoNA)
# add the outcome variable
TrainingCenSca[ ,"classe"] <- Training[ ,"classe"]
```
`r num.predictors` predictors are kept. TrainingNoNA contains only these features.

## The First Iteration of the Random Forest Classifier
```{r warning=FALSE}
# Use randomForest package to predict the classes
modelFitRf <- randomForest(classe ~ ., data = TrainingCenSca, importance=TRUE)
modelFitRf
```
The grown forest consists in 500 trees; at each split a sample of $7 \approx \sqrt 52$ features were tried out.
Indeed each grown tree is built on the basis of a bootstrap sample. Hence on average $(1 - 1/e) * trainingSetSize$ observations are used for the tree. The remaining observations can be used to assess the prediction accuracy: this error rate is called the ouf-of-bag (OOB) error rate. Here it has been estimated to $0.56 \%$ in this basic model.

## Elimination of the Least Significant Features

The importance of the features can be assessed using the varImp function. 
```{r}
varImpPlot(modelFitRf, sort=TRUE, main="Impotance Of Factors");
vi <- (varImp(modelFitRf))
# Let us illustrate the selection with the median. 
i <- 50
threshold <- apply(vi, 2, function(x) quantile(x, probs = i / 100))
keep.var <- apply(vi, 1, function(x) any(x > threshold))
predictors2 <- rownames(vi)[!is.na(keep.var) & keep.var == TRUE]
TrainingSel <- TrainingCenSca[,predictors2]
# add the outcome variable
TrainingSel[,"classe"] <- Training[,"classe"]
modelFitRfSel <- randomForest(classe ~ ., data = TrainingSel, importance=TRUE)
modelFitRfSel
```
The plots depict the relative importance of the 30 most influent features.
The importance of the features is computed with the `varImp` function and, if for some class the importance of a given feature is greater than the median importance for a given class, we keep this feature.
So we end with `r length(predictors2)` features.
The out-of-bag error rate has fallen to 0.49 % (a $`r (.56 - .49)/.56 * 100` \%$-improvement).
More seriously, we should not claim that such a small improvement is statistically significant (at least without further evidence).

The main advantage of this simplification is that it depends on less features:
```{r}
predictors2
```
If we would vary `i` from 50 to 100, we would not find any better quantile threshold than the median on this data set using the seed 1208 before each run of Random Forest. Due to space limitations we do not include this computation in the assignment. When we reduce the number of predictors, the loss of accuracy is very mild, as long as we keep at least 10 to 20 variables as shown by the `rfcv` function results below.

The function `rfcv` computes the out of sample error rate by using cross validation when less and less variables are used.

```{r}
library(ggplot2)
set.seed(1981)
res <- rfcv(TrainingSel[,-dim(TrainingSel)[2]], TrainingSel[,"classe"], cv.fold = 10)
qplot(res$n.var, res$error.cv, xlab="Number of Variables", ylab="Error Rate", main="Cross Validation Error Rate by Number of Variables", geom="line", ylim = c(0,0.1))
```

Since in the other part of the assignment, the prediction accuracy is the sole criterion that matters, we keep our 36-variable model: it does not overfit and provides the least error rate.

## Using 10-Fold Cross Validation for Assessing the Out Of Sample Eror Rate

As noticed on the discussion forum, "random forests" has its own mechanism of cross validation built in: the out of bag error explained above and K-fold cross validation should not be needed.

```{r}
library(cvTools)
set.seed(1208)
K <- 10
R <- 1
errorRates <- rep(NA,K)
folds <- cvFolds(dim(TrainingSel)[1], K = K, R = R)
for (i in 1:K) {
  TestingIndex <- folds$subsets[folds$which == i]
  model <- randomForest(classe ~ ., data=TrainingSel[-TestingIndex,])
  res <- predict(model,TrainingSel[TestingIndex,])
  errorRates[i] <- sum(res != TrainingSel[TestingIndex,"classe"]) / length(res)
}
```

The mean out-of-sample error rate over the 10 folds is `r mean(errorRates) * 100` %. This is slightly higher than the out-of-bag error rate.

## The Final Test

Now our model is ready, and we use it to predict the classes of the test set.
```{r}
TestingNoNA <- Testing[,predictors]
TestingCenSca <- predict(prepro,TestingNoNA)
TestingCenSca[,"classe"] <- Testing[,"classe"]
# check the handcrafted model
ClassesPredicted <- predict(modelFitRfSel, TestingCenSca)
confusionMatrix(ClassesPredicted, TestingCenSca[,"classe"])
```
For the model modelFitRfSel the out-of-bag error rate 0.49 % was remarkably close to the error rate on the test set 0.5 %.

On this data set the out-of-bag estimate of the error rate is really reliable.
